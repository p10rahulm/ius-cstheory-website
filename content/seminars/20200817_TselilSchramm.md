+++
title = "Reconciling Statistical Queries and the Low Degree Likelihood Ratio"
author = "<a href="https://tselilschramm.org/" target="_blank">Tselil Schramm, Stanford</a>"
date = "2020-08-17T21:00:00+05:30"
date_end = "2020-08-17T22:00:00+05:30"
location = "<a href="#">Location: Virtual Seminar</a>"
notes = "Joint <a href = "http://www.arc.gatech.edu/" target = "_blank">ARC seminar</a> and <a href='https://polyalg.csa.iisc.ac.in/'>Indo-US virtual center seminar</a>"
+++

<b>Abstract:</b> In many high-dimensional statistics problems, we observe information-computation tradeoffs: given access to more data, statistical estimation and inference tasks require fewer computational resources. Though this phenomenon is ubiquitous, we lack rigorous evidence that it is inherent. In the current day, to prove that a statistical estimation task is computationally intractable, researchers must prove lower bounds against each type of algorithm, one by one, resulting in a "proliferation of lower bounds". We scientists dream of a more general theory which unifies these lower bounds and explains computational intractability in an algorithm-independent way.
<br><br>
In this talk, I will make one small step towards realizing this dream. I will demonstrate general conditions under which two popular frameworks yield the same information-computation tradeoffs for high-dimensional hypothesis testing: the first being statistical queries in the "SDA" framework, and the second being hypothesis testing with low-degree hypothesis tests, also known as the low-degree-likelihood ratio. Our equivalence theorems capture numerous well-studied high-dimensional learning problems: sparse PCA, tensor PCA, community detection, planted clique, and more.
<br><br>
Based on joint work with Matthew Brennan, Guy Bresler, Samuel B. Hopkins and Jerry Li.